{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-09 08:35:07 [__init__.py:235] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.54.1. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.635 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel,FastLanguageModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utility import TTTDataset_iter, seed_worker\n",
    "from torch.utils.data import DataLoader\n",
    "from utility import load_grouped_data\n",
    "from peft import PeftModel\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\",\n",
    "    model_name = \"unsloth/Qwen3-4B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    # max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_weight = nn.Parameter(torch.load('./Model/Gwen4B_lm_head.pth').T)\n",
    "# lm_head_weight = nn.Parameter(torch.load('./Model/Gwen8B_lm_head.pth').T)\n",
    "lm_head_weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be8c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, holdout_data = load_grouped_data()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    TTTDataset_iter(train_data, holdout_data, tokenizer, samples_per_epoch=1000),\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    worker_init_fn=seed_worker,\n",
    "    collate_fn=lambda x: x[0]\n",
    ")\n",
    "# for test_idx_info, input_ids, vi_index, labels in dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3ba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de811703",
   "metadata": {},
   "source": [
    "#### Fine-tune lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc07711",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accumulation_steps = 32\n",
    "lr = 1e-5\n",
    "clip = 1e-3\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f133e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [lm_head_weight]\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction='none')\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca5934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.5570830078125, val_loss: 0.5913955078125\n",
      "Epoch 1 train_loss: 0.6126015625, val_loss: 0.58017578125\n",
      "Epoch 2 train_loss: 0.623546875, val_loss: 0.566310546875\n",
      "Epoch 3 train_loss: 0.5957783203125, val_loss: 0.590337890625\n",
      "Epoch 4 train_loss: 0.6101357421875, val_loss: 0.5660537109375\n",
      "Epoch 5 train_loss: 0.5791826171875, val_loss: 0.543955078125\n",
      "Epoch 6 train_loss: 0.586201171875, val_loss: 0.5533984375\n",
      "Epoch 7 train_loss: 0.62674609375, val_loss: 0.568876953125\n",
      "Epoch 8 train_loss: 0.5879658203125, val_loss: 0.56141015625\n",
      "Epoch 9 train_loss: 0.5730390625, val_loss: 0.5651708984375\n",
      "Time taken: 6.346543021996816 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            with torch.no_grad(): # as we are training the lm_head only.\n",
    "                output = model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0]\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd1bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm_head_weight, 'Model/lm_head_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86dcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e4abe7",
   "metadata": {},
   "source": [
    "#### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947689ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accumulation_steps = 32\n",
    "lr = 6e-7\n",
    "clip = 6e-5\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4aaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "lm_head_weight.requires_grad_(True);\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "trainable_params.append(lm_head_weight)\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr) \n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction='none')\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40227ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.5915966796875, val_loss: 0.56445703125\n",
      "Epoch 1 train_loss: 0.5783642578125, val_loss: 0.5507529296875\n",
      "Epoch 2 train_loss: 0.5625537109375, val_loss: 0.588408203125\n",
      "Epoch 3 train_loss: 0.54944921875, val_loss: 0.57584765625\n",
      "Epoch 4 train_loss: 0.5615791015625, val_loss: 0.5427177734375\n",
      "Epoch 5 train_loss: 0.5929501953125, val_loss: 0.569513671875\n",
      "Epoch 6 train_loss: 0.6152080078125, val_loss: 0.57633984375\n",
      "Epoch 7 train_loss: 0.5685703125, val_loss: 0.5600537109375\n",
      "Epoch 8 train_loss: 0.56568359375, val_loss: 0.5652109375\n",
      "Epoch 9 train_loss: 0.572830078125, val_loss: 0.555408203125\n",
      "Time taken: 20.43752895196279 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0]\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff674cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.5560966796875, val_loss: 0.5601484375\n",
      "Epoch 1 train_loss: 0.5756650390625, val_loss: 0.5670615234375\n",
      "Epoch 2 train_loss: 0.562619140625, val_loss: 0.57713671875\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0]\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Model/merged_model4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd43de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "# model = PeftModel.from_pretrained(model, \"Model/merged_model4b\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d6f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
