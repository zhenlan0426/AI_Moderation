{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-05 16:41:51 [__init__.py:235] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.54.1. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.635 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utility import TTTDataset, seed_worker\n",
    "from torch.utils.data import DataLoader\n",
    "from utility import load_grouped_data\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    model_name = \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_weight = nn.Parameter(torch.load('./Model/Gwen8B_lm_head_base.pth').T)\n",
    "# lm_head_weight = nn.Parameter(torch.load('./Model/Gwen8B_lm_head.pth').T)\n",
    "lm_head_weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be8c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, holdout_data = load_grouped_data()\n",
    "\n",
    "loader = DataLoader(\n",
    "    TTTDataset(train_data, holdout_data, tokenizer, samples_per_epoch=1000),\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    worker_init_fn=seed_worker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce95830",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_loader = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c138b3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, vi_index, labels = next(iter_loader)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9eec348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 131])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea62599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 59,  69, 130])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6344de66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 25, 25])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0, vi_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edaa7039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffdc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de811703",
   "metadata": {},
   "source": [
    "#### Fine-tune lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc07711",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "accumulation_steps = 64\n",
    "lr = 1e-5\n",
    "clip = 1e-3\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f133e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [lm_head_weight]\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca5934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.8505517650320354\n",
      "Epoch 1 loss: 0.7960620803043371\n",
      "Epoch 2 loss: 0.7578240512567768\n",
      "Time taken: 4.878338877360026 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (row_id, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        row_id = int(row_id)\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            with torch.no_grad(): # as we are training the lm_head only.\n",
    "                output = model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits[:2], labels) # first 2 tokens are used for training, (N, C), (N,)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            if vi_index.shape[0] == 3:\n",
    "                prob = torch.softmax(logits[2], dim=0)[1].item()\n",
    "                prob_list[row_id].append(prob)\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} loss: {train_loss / (i+1)}\")\n",
    "    train_loss = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1bf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947689ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
