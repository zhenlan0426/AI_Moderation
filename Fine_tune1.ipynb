{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-10 07:42:12 [__init__.py:235] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.54.1. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.635 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel,FastLanguageModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utility import TTTDataset_iter, seed_worker\n",
    "from torch.utils.data import DataLoader\n",
    "from utility import load_grouped_data\n",
    "from peft import PeftModel\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\",\n",
    "    model_name = \"unsloth/Qwen3-4B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    # max_seq_length = 8192, # Choose any for long context!\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_weight = nn.Parameter(torch.load('./Model/Gwen4B_lm_head.pth').T)\n",
    "# lm_head_weight = nn.Parameter(torch.load('./Model/Gwen8B_lm_head.pth').T)\n",
    "lm_head_weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, holdout_data = load_grouped_data()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    TTTDataset_iter(train_data, holdout_data, tokenizer, samples_per_epoch=2000),\n",
    "    batch_size=1,\n",
    "    # num_workers=4,\n",
    "    worker_init_fn=seed_worker,\n",
    "    collate_fn=lambda x: x[0]\n",
    ")\n",
    "# for test_idx_info, input_ids, vi_index, labels in dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e0f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de811703",
   "metadata": {},
   "source": [
    "#### Fine-tune lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc07711",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accumulation_steps = 64\n",
    "lr = 2e-4\n",
    "clip = 2e-2\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05f133e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [lm_head_weight]\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction='none')\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            with torch.no_grad(): # as we are training the lm_head only.\n",
    "                output = model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0] / accumulation_steps\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum * accumulation_steps / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd1bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm_head_weight, 'Model/lm_head_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86dcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e4abe7",
   "metadata": {},
   "source": [
    "#### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947689ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accumulation_steps = 64\n",
    "lr = 2e-5\n",
    "clip = 2e-3\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4aaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "lm_head_weight.requires_grad_(True);\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "trainable_params.append(lm_head_weight)\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr) \n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction='none')\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40227ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.5915966796875, val_loss: 0.56445703125\n",
      "Epoch 1 train_loss: 0.5783642578125, val_loss: 0.5507529296875\n",
      "Epoch 2 train_loss: 0.5625537109375, val_loss: 0.588408203125\n",
      "Epoch 3 train_loss: 0.54944921875, val_loss: 0.57584765625\n",
      "Epoch 4 train_loss: 0.5615791015625, val_loss: 0.5427177734375\n",
      "Epoch 5 train_loss: 0.5929501953125, val_loss: 0.569513671875\n",
      "Epoch 6 train_loss: 0.6152080078125, val_loss: 0.57633984375\n",
      "Epoch 7 train_loss: 0.5685703125, val_loss: 0.5600537109375\n",
      "Epoch 8 train_loss: 0.56568359375, val_loss: 0.5652109375\n",
      "Epoch 9 train_loss: 0.572830078125, val_loss: 0.555408203125\n",
      "Time taken: 20.43752895196279 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0] / accumulation_steps\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum * accumulation_steps / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aff674cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.5560966796875, val_loss: 0.5601484375\n",
      "Epoch 1 train_loss: 0.5756650390625, val_loss: 0.5670615234375\n",
      "Epoch 2 train_loss: 0.562619140625, val_loss: 0.57713671875\n",
      "Epoch 3 train_loss: 0.558087890625, val_loss: 0.557912109375\n",
      "Epoch 4 train_loss: 0.58446484375, val_loss: 0.5678203125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m loss = loss_fn(logits, labels) \u001b[38;5;66;03m# first token is used for training\u001b[39;00m\n\u001b[32m     12\u001b[39m train_loss = loss[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrain_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# tracking the loss\u001b[39;00m\n\u001b[32m     16\u001b[39m train_loss_accum += train_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0]\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Model/merged_model4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd43de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "# model = PeftModel.from_pretrained(model, \"Model/merged_model4b\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d6f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
