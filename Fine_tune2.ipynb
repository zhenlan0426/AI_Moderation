{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060059dc",
   "metadata": {},
   "source": [
    "Use FastLanguageModel and incremental clean URL etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-16 10:16:53 [__init__.py:235] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.54.1. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.635 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utility import TTTDataset_iter\n",
    "from torch.utils.data import DataLoader\n",
    "from utility import load_grouped_data\n",
    "from peft import PeftModel\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\",\n",
    "    model_name = \"unsloth/Qwen3-4B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-1.7B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    max_seq_length = 8192, # Choose any for long context!\n",
    "    # resize_model_vocab = 80999, \n",
    "    load_in_4bit = True,\n",
    ")\n",
    "# model.model.embed_tokens.load_state_dict({'weight':torch.load('Model/reduced_embedding.pt')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185ce874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2308, 7414]\n"
     ]
    }
   ],
   "source": [
    "# save the lm_head for yes and no\n",
    "index = [tokenizer.encode(\" No\")[0], tokenizer.encode(\" Yes\")[0]]\n",
    "print(index)\n",
    "torch.save(model.lm_head.weight[index], './Model/Gwen_4B_lm_head.pth')\n",
    "# lm_head_weight = nn.Parameter(torch.load('./Model/Gwen1_7B_lm_head.pth').T)\n",
    "lm_head_weight = nn.Parameter(torch.load('Model/Gwen4B_lm_head.pth').T)\n",
    "# lm_head_weight = nn.Parameter(torch.load('Model/lm_head_weight.pth'))\n",
    "\n",
    "# # lm_head_weight = nn.Parameter(torch.load('./Model/Gwen8B_lm_head.pth').T)\n",
    "lm_head_weight.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be8c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, holdout_data = load_grouped_data()\n",
    "# old_to_new = torch.load(\"Model/vocab_mapping.pt\")\n",
    "dataloader = DataLoader(\n",
    "    TTTDataset_iter(train_data, holdout_data, tokenizer, None, samples_per_epoch=2000),\n",
    "    batch_size=1,\n",
    "    collate_fn=lambda x: x[0]\n",
    ")\n",
    "\n",
    "# for test_idx_info, input_ids, vi_index, labels in dataloader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e0f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de811703",
   "metadata": {},
   "source": [
    "#### Fine-tune lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc07711",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accumulation_steps = 64\n",
    "lr = 1e-4\n",
    "clip = 1e-2\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f133e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [lm_head_weight]\n",
    "optimizer = torch.optim.Adam(trainable_params,lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction='none')\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3344bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            with torch.no_grad(): # as we are training the lm_head only.\n",
    "                output = model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0] / accumulation_steps\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum * accumulation_steps / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd1bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm_head_weight, 'Model/lm_head_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86dcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e4abe7",
   "metadata": {},
   "source": [
    "#### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947689ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "accumulation_steps = 64\n",
    "lr = 2e-5\n",
    "clip = 2e-3\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4aaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "505\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "lm_head_weight.requires_grad_(True);\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "trainable_params.append(lm_head_weight)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr = lr) \n",
    "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing, reduction='none')\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cda0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.50803857421875, val_loss: 0.54365478515625\n",
      "Epoch 1 train_loss: 0.4759853515625, val_loss: 0.49885546875\n",
      "Epoch 2 train_loss: 0.44575244140625, val_loss: 0.46722314453125\n",
      "Epoch 3 train_loss: 0.41596630859375, val_loss: 0.46640234375\n",
      "Epoch 4 train_loss: 0.39271630859375, val_loss: 0.4431669921875\n",
      "Epoch 5 train_loss: 0.37871630859375, val_loss: 0.4225458984375\n",
      "Epoch 6 train_loss: 0.3790576171875, val_loss: 0.4259345703125\n",
      "Epoch 7 train_loss: 0.35674755859375, val_loss: 0.40379736328125\n",
      "Epoch 8 train_loss: 0.34932080078125, val_loss: 0.4596162109375\n",
      "Epoch 9 train_loss: 0.3409609375, val_loss: 0.41560888671875\n",
      "Time taken: 44.176959780852 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0] / accumulation_steps\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum * accumulation_steps / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb9d94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss: 0.34118310546875, val_loss: 0.38859033203125\n",
      "Epoch 1 train_loss: 0.33535400390625, val_loss: 0.39411572265625\n",
      "Epoch 2 train_loss: 0.3332578125, val_loss: 0.3985576171875\n",
      "Epoch 3 train_loss: 0.31943701171875, val_loss: 0.37522705078125\n",
      "Epoch 4 train_loss: 0.31520947265625, val_loss: 0.38720166015625\n",
      "Epoch 5 train_loss: 0.3197294921875, val_loss: 0.403220703125\n",
      "Epoch 6 train_loss: 0.327759765625, val_loss: 0.42085986328125\n",
      "Epoch 7 train_loss: 0.3104892578125, val_loss: 0.393880859375\n",
      "Epoch 8 train_loss: 0.31201025390625, val_loss: 0.41192041015625\n",
      "Epoch 9 train_loss: 0.3086748046875, val_loss: 0.38175048828125\n",
      "Time taken: 47.241845679283145 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0] / accumulation_steps\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum * accumulation_steps / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646883e",
   "metadata": {},
   "source": [
    "Reduce learning rate by 4 and use torch.float16 to be consistent with Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "val_loss_accum = 0\n",
    "prob_list = defaultdict(list)\n",
    "for epoch in range(5):\n",
    "    for i, (test_idx_info, input_ids, vi_index, labels) in enumerate(dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            input_ids, vi_index, labels = input_ids.to('cuda'), vi_index.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = output.last_hidden_state[0, vi_index] @ lm_head_weight # (# of Violation, 4096) @ (4096, 2) -> (# of Violation, 2)\n",
    "            loss = loss_fn(logits, labels) # first token is used for training\n",
    "            train_loss = loss[0] / accumulation_steps\n",
    "            train_loss.backward()\n",
    "\n",
    "            # tracking the loss\n",
    "            train_loss_accum += train_loss.item()\n",
    "            val_loss_accum += loss[1].item()\n",
    "            # TODO: track the probability of the test example in nested list\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                clip_grad_value_(trainable_params,clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch} train_loss: {train_loss_accum * accumulation_steps / (i+1)}, val_loss: {val_loss_accum / (i+1)}\")\n",
    "    train_loss_accum = 0\n",
    "    val_loss_accum = 0\n",
    "print(f\"Time taken: {(time.time() - start_time)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Model/merged_model4b\")\n",
    "torch.save(lm_head_weight, 'Model/lm_head_weight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd43de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "model = PeftModel.from_pretrained(model, \"Model/merged_model4b\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637ff10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
