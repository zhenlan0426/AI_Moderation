- remove url link with link + domain
- to use subreddit or not
- rules + positive/negative examples + test examples, backprop from positive/negative examples and keep track of test example logit predictions
- text normalization
- iterative data cleaning. top false positives and false negatives by model -> GPT rerate GT -> retrain model -> repeat
- due to target noise, introduce soft labels
- maybe URL is enough, no need for domain as there are too many domains?
- bias or not for lm_head?
- Optional once-only pseudo-labeling: add high-confidence test cases (e.g., p>0.9) into the support pool with weight < 1.0 for one extra sweep. Stop after one iteration to avoid confirmation bias.
- For each test prompt, take the logits at the final “Violation:” position, compute softmax p over {No, Yes}, and minimize prediction entropy H(p) = −∑ p log p.
- remove UNK instead of using averaged vector as fallback
- at inference time, positive/negative examples can be much fewer than test examples, fine-tune can lead to overfitting. maybe scale the learning rate by the ratio of positive/negative examples to test examples?
- focus on the most unconfident examples (middle 25% of the test set)
- per rule TTT or one TTT
- train on pos/neg and use KV cache to do inference to save VRAM
- ARC without pretraining
- without header
- ✅ data2 and data3 needs to be split into positive/negative examples